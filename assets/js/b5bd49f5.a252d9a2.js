"use strict";(self.webpackChunkntu_dsai_github_io=self.webpackChunkntu_dsai_github_io||[]).push([[7e3],{3905:function(e,t,n){n.d(t,{Zo:function(){return c},kt:function(){return g}});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)n=o[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=r.createContext({}),u=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=u(e.components);return r.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},p=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),p=u(n),g=a,h=p["".concat(l,".").concat(g)]||p[g]||d[g]||o;return n?r.createElement(h,i(i({ref:t},c),{},{components:n})):r.createElement(h,i({ref:t},c))}));function g(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,i=new Array(o);i[0]=p;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:a,i[1]=s;for(var u=2;u<o;u++)i[u]=n[u];return r.createElement.apply(null,i)}return r.createElement.apply(null,n)}p.displayName="MDXCreateElement"},98:function(e,t,n){n.r(t),n.d(t,{assets:function(){return c},contentTitle:function(){return l},default:function(){return g},frontMatter:function(){return s},metadata:function(){return u},toc:function(){return d}});var r=n(3117),a=n(102),o=(n(7294),n(3905)),i=["components"],s={id:"sc4002-natural-language-processing",slug:"sc4002-natural-language-processing",sidebar_position:13,title:"SC4002 Natural Language Processing",description:"Creating and understanding Natural Language Models",keywords:["natural","language","procesisng","models","ntu","scse","module","course"]},l=void 0,u={unversionedId:"Module-Review/MPEs/sc4002-natural-language-processing",id:"Module-Review/MPEs/sc4002-natural-language-processing",title:"SC4002 Natural Language Processing",description:"Creating and understanding Natural Language Models",source:"@site/docs/Module-Review/MPEs/SC4002 Natural Language Processing.md",sourceDirName:"Module-Review/MPEs",slug:"/Module-Review/MPEs/sc4002-natural-language-processing",permalink:"/docs/Module-Review/MPEs/sc4002-natural-language-processing",editUrl:"https://github.com/NTU-DSAI/NTU-DSAI.github.io/tree/master/docs/Module-Review/MPEs/SC4002 Natural Language Processing.md",tags:[],version:"current",sidebarPosition:13,frontMatter:{id:"sc4002-natural-language-processing",slug:"sc4002-natural-language-processing",sidebar_position:13,title:"SC4002 Natural Language Processing",description:"Creating and understanding Natural Language Models",keywords:["natural","language","procesisng","models","ntu","scse","module","course"]},sidebar:"tutorialSidebar",previous:{title:"SC4001 Neural Networks and Deep Learning",permalink:"/docs/Module-Review/MPEs/sc4001-neural-networks-and-deep-learning"},next:{title:"SC4020 Data Analytics and Mining",permalink:"/docs/Module-Review/MPEs/sc4020-data-analytics-and-mining"}},c={},d=[{value:"Course Summary",id:"course-summary",level:2},{value:"Workload",id:"workload",level:2},{value:"Projects",id:"projects",level:2},{value:"Things to take note of",id:"things-to-take-note-of",level:2},{value:"Conclusion",id:"conclusion",level:2}],p={toc:d};function g(e){var t=e.components,n=(0,a.Z)(e,i);return(0,o.kt)("wrapper",(0,r.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h2",{id:"course-summary"},"Course Summary"),(0,o.kt)("p",null,"This course teaches about understanding Natural Languages and how Statistical, as well as deep learning models can be built for text summarisation, abstraction, prediction and translation. The course is taught by Prof Aixin for the first half and Prof Joty for the second. Both of them are well versed in the content and the lessons are fairly interesting. Prof Joty works at SalesForce on his own NLP projects so he brings that unique perspectice to the classes. The content covered for this course includes:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Regular Expressions"),(0,o.kt)("li",{parentName:"ol"},"Words and Inducers"),(0,o.kt)("li",{parentName:"ol"},"N-grams and Language Models"),(0,o.kt)("li",{parentName:"ol"},"Part-of-Speech Tagging"),(0,o.kt)("li",{parentName:"ol"},"Hidden Markov Models"),(0,o.kt)("li",{parentName:"ol"},"Parsing"),(0,o.kt)("li",{parentName:"ol"},"Word2Vec"),(0,o.kt)("li",{parentName:"ol"},"CNNs, RNNs and LSTMs"),(0,o.kt)("li",{parentName:"ol"},"Seq-2-Seq Models")),(0,o.kt)("h2",{id:"workload"},"Workload"),(0,o.kt)("p",null,"Workload is fairly light for most of the semester, this mod is more project heavy as there is no final exam but there are 2 quizzes. There are tutorials as well which the Prof will go through. For the second half, the tutorials have a guided version in case you are lost and need some help completing the assignment."),(0,o.kt)("h2",{id:"projects"},"Projects"),(0,o.kt)("p",null,"There are 2 group projects for this module. The first is more of a research and analysis type of project as the first half of the course mostly deals with understanding natural languages on a whole. Since you have 4-5 persons per group, you can divide-and-conquer the whole project. Prof Aixin is particular about the format of the submission of the report so do use ",(0,o.kt)("a",{parentName:"p",href:"https://www.overleaf.com/"},"Overleaf")," as it provides the template that he has specifically requested."),(0,o.kt)("p",null,"The second project is creating 2 NLP models, one Text Generation and one Named Entity Recognition. This can also be achieved by pairing up in your group and creating the language models. Do remember to take note of what other members in your group are doing so that you can keep on top of the topics that you have learnt."),(0,o.kt)("h2",{id:"things-to-take-note-of"},"Things to take note of"),(0,o.kt)("p",null,"As you will need to train language models for this project, be prepared to spend quite some time fiddling with the hyperparameters. It will be time consuming especially if you are using Google Colab as you will not have access to high powered GPUs to make the computations go by faster. If you have your own Gaming PC with at least a RTX 3060, you will be able to save yourself some time."),(0,o.kt)("h2",{id:"conclusion"},"Conclusion"),(0,o.kt)("p",null,"Overall this module is pretty fun and if you're really interested in persuing NLP in the future, it does give you a glimpse into what it is like to build and run NLP models. In my experience, you can get more practical NLP experience in ",(0,o.kt)("a",{parentName:"p",href:"/docs/Module-Review/MPEs/sc4001-neural-networks-and-deep-learning"},"Neural Networks and Deep Learning")," as you will be able to test your NLP skills on a real dataset and actually create Seq-2-Seq models. But this course does definitely teach you how to understand and interpret natural languages."))}g.isMDXComponent=!0}}]);